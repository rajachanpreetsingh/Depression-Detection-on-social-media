{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas for data manipulation in rows and columns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #importing stopwords from nltk\n",
    "stop_words = stopwords.words('english')#loading stop words\n",
    "import numpy as np #numpy for matrix manipulation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#load tfidf vector\n",
    "import re #import regular expression for text manipulation\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import metrics, svm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "import os\n",
    "import conda\n",
    "\n",
    "conda_file_directories = conda.__file__\n",
    "conda_dirs_ = conda_file_directories.split('lib')[0]\n",
    "proj_lib_conda = os.path.join(os.path.join(conda_dirs_, 'share'), 'proj')\n",
    "os.environ[\"PROJ_LIB\"] = proj_lib_conda\n",
    "import matplotlib\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import folium\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import itertools\n",
    "import warnings\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score \n",
    "warnings.filterwarnings('ignore')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import requests\n",
    "import plotly\n",
    "import chart_studio.plotly as cpy\n",
    "import sklearn\n",
    "import datetime\n",
    "import descartes\n",
    "from shapely.geometry import Point, Polygon\n",
    "import plotly.graph_objs as go\n",
    "%matplotlib inline\n",
    "import unicodecsv\n",
    "from unidecode import unidecode\n",
    "import pyLDAvis.gensim\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.models import Word2Vec #generating word2vec embeddings\n",
    "from geopy.geocoders import Nominatim\n",
    "from IPython.display import SVG\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation,GlobalMaxPool1D, Bidirectional, MaxPooling1D #For Neural Networks\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix,recall_score\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading depressed Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed=pd.read_csv('dataset/df_depressed_tm.csv',index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_depressed.columns #checking depression tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Checking for duplicate Records in Depressed Tweets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed = df_depressed.drop_duplicates(subset=['tweet'], keep='first')\n",
    "df_depressed = df_depressed.drop_duplicates(subset=['username'], keep='first')\n",
    "df_depressed.dropna(subset=['date', 'time'],inplace=True)\n",
    "df_depressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading Happy Tweets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_happy = pd.read_csv('dataset/df_tweets_happy_tm.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_happy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_happy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_happy = df_tweets_happy.drop_duplicates(subset=['tweet'], keep='first')\n",
    "df_tweets_happy = df_tweets_happy.drop_duplicates(subset=['username'], keep='first')\n",
    "df_tweets_happy.dropna(subset=['date', 'time'],inplace=True)\n",
    "df_tweets_happy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Checking Look of Samples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tweets_happy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed['target'] = 1\n",
    "df_depressed['category'] = 'depressed'\n",
    "df_tweets_happy['category'] = 'non-depressed'\n",
    "df_tweets_happy['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Downloading query </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name=happy_users[uc]\n",
    "#uc=uc+1\n",
    "#print(\"twint -u \"+name+\" --since \\\"2020-01-01 00:00:00\\\" -o Downloads/thesis_dataset/happy_user\"+str(fc)+\" --csv\")\n",
    "#fc=fc+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Combining Depressed and Happy Tweets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined=[df_depressed,df_tweets_happy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets=pd.concat(all_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.drop_duplicates(subset=['tweet'], keep='first')\n",
    "all_tweets = all_tweets.drop_duplicates(subset=['username'], keep='first')\n",
    "all_tweets.dropna(subset=['date', 'time'],inplace=True)\n",
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tweets.to_csv('all_tweets_cc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tweets=pd.read_csv('all_tweets_cc.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Visualization </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preparing Data for visualization </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_extracted_one=pd.read_csv('dataset/all_user_details_all.csv')\n",
    "users_extracted_two=pd.read_csv('dataset/all_user_details_all_two.csv')\n",
    "users_extracted_three=pd.read_csv('dataset/all_user_details_all_three.csv')\n",
    "users_extracted_four=pd.read_csv('dataset/all_user_details_all_four.csv')\n",
    "users_extracted_five=pd.read_csv('dataset/all_user_details_all_five.csv')\n",
    "users_extracted_six=pd.read_csv('dataset/all_user_details_all_six.csv')\n",
    "users_extracted_seven=pd.read_csv('dataset/all_user_details_all_seven.csv')\n",
    "users_extracted_eight=pd.read_csv('dataset/all_user_details_all_eight.csv')\n",
    "users_extracted_nine=pd.read_csv('dataset/all_user_details_all_nine.csv')\n",
    "users_extracted_ten=pd.read_csv('dataset/all_user_details_all_ten.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_info_frame=[users_extracted_one,users_extracted_two,users_extracted_three,users_extracted_four,\n",
    "                    users_extracted_five,users_extracted_six,users_extracted_seven,users_extracted_eight,\n",
    "                    users_extracted_nine,users_extracted_ten]\n",
    "df_users_info=pd.concat(df_users_info_frame)\n",
    "df_users_info = df_users_info.drop_duplicates(subset=['person_username'], keep='first')\n",
    "df_users_info.drop(['person_time_zone', 'tweet_latitude','tweet_longitude','in_reply_to_screen_name'], inplace=True, axis=1)\n",
    "df_users_info.drop(['urls', 'hashtags','tweet_contributions','media_type','user_mentions'], inplace=True, axis=1)\n",
    "df_users_info['lower_username']=df_users_info['person_username'].str.lower()\n",
    "all_tweets['lower_username']=all_tweets['username'].str.lower()\n",
    "df_select=all_tweets.loc[all_tweets['lower_username'].isin(df_users_info['lower_username'])]\n",
    "df_select_info=df_users_info.loc[df_users_info['lower_username'].isin(df_select['lower_username'])]\n",
    "df_select_info.to_csv('users_info_all_combined.csv')\n",
    "df_select.to_csv('users_having_info.csv')\n",
    "df_vis = pd.concat([df_select.reset_index(drop=True),df_select_info.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['category'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_vis['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_targets=all_tweets['target']\n",
    "sns.countplot(tweet_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Visualization For Location of Users</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpy.sign_in('Rajachanpreetsingh', 'MaAJQBXs5Ouv7Y9CnjFM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['person_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cnt_loc = df_vis['person_location'].value_counts()\n",
    "#cnt_loc.reset_index()\n",
    "#cnt_loc = cnt_loc[:20,]\n",
    "#trace_loc = go.Bar(x = cnt_loc.index,\n",
    "#                y = cnt_loc.values,\n",
    "#                name = \"Number of Tweets in dataset according to Location\",\n",
    "#                marker = dict(color = 'rgba(194, 64, 63, 0.5)',\n",
    "#                             line=dict(color='rgb(0,0,0)',width=2.0)),\n",
    "#                )\n",
    "#\n",
    "#graph_data = [trace_loc]\n",
    "#graph_layout = go.Layout(barmode = \"group\",title = 'Number of Tweets in dataset according to Location')\n",
    "#graph_fig = go.Figure(data = graph_data, layout = graph_layout)\n",
    "#cpy.iplot(graph_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis2 = df_vis.dropna(axis=0, subset=['person_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_list=list(df_vis2['person_username'])\n",
    "#locations_list=list(df_vis2['person_location'])\n",
    "#latitude_list=[]\n",
    "#longitude_list=[]\n",
    "#usernames_loc=[]\n",
    "#missed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('document.csv','a') as fd:\n",
    "#    fd.write(myCsvRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('lat_long_info.csv','a') as loc_file:\n",
    " #   writerfile_info = csv.writer(loc_file, delimiter = ',', quotechar = '\"')\n",
    "  #  writerfile_info.writerow(['username','location','latitude','longitude'])\n",
    "   # for i in range(80000,85000): \n",
    "   #     geolocator = Nominatim(user_agent=\"gettingold locations\")\n",
    "   #     try: \n",
    "   #         location = geolocator.geocode(locations_list[i])\n",
    "   #         latitude_list.append(location.latitude)\n",
    "   #         longitude_list.append(location.longitude)\n",
    "   #         usernames_loc.append(users_list[i])\n",
    "   #         writerfile_info.writerow([users_list[i],locations_list[i],location.latitude,location.longitude])\n",
    "   #     except:\n",
    "   #         missed.append(users_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info = pd.read_csv('dataset/lat_long_info.csv')\n",
    "locs_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info['latitude'] = pd.to_numeric(locs_info['latitude'], errors='coerce')\n",
    "locs_info['longitude'] = pd.to_numeric(locs_info['longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info['latitude'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info['longitude'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info = locs_info.dropna(axis=0, subset=['latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_val=106\n",
    "plt.figure(figsize=(2600/dpi_val, 1800/dpi_val), dpi=dpi_val)\n",
    "\n",
    "all_tw_m=Basemap(llcrnrlon=-180, llcrnrlat=-65,urcrnrlon=180,urcrnrlat=80)\n",
    "all_tw_m.drawmapboundary(fill_color='#A6CAE0', linewidth=0)\n",
    "all_tw_m.fillcontinents(color='#00a000', alpha=0.3)\n",
    "all_tw_m.drawcoastlines(linewidth=0.1, color=\"black\")\n",
    " \n",
    "\n",
    "all_tw_m.scatter(locs_info['longitude'], locs_info['latitude'], alpha=0.7, c='blue', cmap=\"Set1\")\n",
    " \n",
    "plt.text( -169, -58,'Tweets Collected from all over the world', ha='left', va='bottom', size=10, color='#555555' )\n",
    " \n",
    "plt.savefig('all_tweeets_loc.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Obtaining Depressed Users</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_frame=[]\n",
    "locs_users=list(locs_info['username'])\n",
    "for n in locs_users:\n",
    "    tempdf = df_vis.loc[(df_vis['person_username'] == n) & (df_vis['target'] == 1)]\n",
    "    vis_dep_frame.append(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_map = pd.concat(vis_dep_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_dep_map.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dep_map_users = list(vis_dep_map['person_username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_frame = []\n",
    "for n in final_dep_map_users:\n",
    "    tempdf = locs_info.loc[(locs_info['username']==n)]\n",
    "    vis_dep_frame.append(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_map_final = pd.concat(vis_dep_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dep_map_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualizing Depressed users on Map </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_val=106\n",
    "plt.figure(figsize=(2600/dpi_val, 1800/dpi_val), dpi=dpi_val)\n",
    "\n",
    "all_tw_m=Basemap(llcrnrlon=-180, llcrnrlat=-65,urcrnrlon=180,urcrnrlat=80)\n",
    "all_tw_m.drawmapboundary(fill_color='#A6CAE0', linewidth=0)\n",
    "all_tw_m.fillcontinents(color='#00a000', alpha=0.5)\n",
    "all_tw_m.drawcoastlines(linewidth=0.1, color=\"black\")\n",
    " \n",
    "\n",
    "all_tw_m.scatter(vis_dep_map_final['longitude'], vis_dep_map_final['latitude'], alpha=0.7, c='red', cmap=\"Set1\")\n",
    " \n",
    "plt.text( -169, -58,'Tweets Collected from all over the world for Depression Category', ha='left', va='bottom', size=10, color='#555555' )\n",
    " \n",
    "plt.savefig('dep_tweeets_loc.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targ_one_lats=[]\n",
    "#targ_one_longs=[]\n",
    "#targ_one_unames=[]\n",
    "#cc=0\n",
    "#for i in range(len(vis_users)):\n",
    "#    if (locs_users[i] == vis_users[cc]) and (vis_targs[cc]==1):\n",
    "##        targ_one_lats.append(vis_dep_lat[i])\n",
    " #       targ_one_longs.append(vis_dep_long[i])\n",
    " #       targ_one_unames.append(vis_users[cc])\n",
    " #       cc=cc+1\n",
    " #   else:\n",
    " #       cc=cc+1\n",
    "#df_dep_vis=pd.concat(dep_vis_frame)\n",
    "#df_dep_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Distribution of Hours based on Category Depressed/Non Depressed</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_dist=pd.crosstab(df_vis.hour,df_vis.category)\n",
    "hours_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df_vis['hour'],columns = df_vis['category']).plot(kind='bar',color=['red','blue'],figsize=(10, 6),alpha=0.6,rot=0,title=\"Tweets posted based on hours\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Busiest Hours of the user </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_count  = df_vis['hour'].value_counts()\n",
    "hour_count = hour_count[:10,]\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(hour_count.index, hour_count.values, alpha=1)\n",
    "plt.title('Tweets posted by Users according to hours')\n",
    "plt.ylabel('Number of Occurrences', fontsize=13)\n",
    "plt.xlabel('Hours', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Percentage Split of Hours </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_hours=hours_dist.apply(lambda y: y / y.sum() * 100, axis=0)\n",
    "percentage_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Posts Based on Years </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_yearwise=pd.crosstab(df_vis.year,df_vis.category)\n",
    "posts_yearwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df_vis['year'],columns = df_vis['category']).plot(kind='bar',color=['red','blue'],figsize=(8, 6),alpha=0.6,rot=0,title=\"Tweets posted based on years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Plotting Users with maximum following count</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot(x_,y_,z_,xlabel_,title_,fig_size_=(10,10)):\n",
    "    plt.figure(figsize=fig_size_)    \n",
    "    sns.barplot(x=x_,y=y_,palette = 'terrain',orient='h',order=y_)\n",
    "    category=z_.values\n",
    "    i=0\n",
    "    for j,v in enumerate(x_):\n",
    "        #plt.text(0.5,j,v,color='k',fontsize=10)\n",
    "        plt.text(0.6,j,category[i],color='b',fontsize=10)\n",
    "        i=i+1\n",
    "    plt.title(title_,fontsize=18)\n",
    "    plt.xlabel(xlabel_,fontsize =13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_follower = df_vis.loc[:,['username','person_followers_count','category']]\n",
    "df_us_follower.sort_values(by='person_followers_count',ascending=False,inplace=True)\n",
    "df_us_follower.drop_duplicates(subset='username',keep='first',inplace=True)\n",
    "df_count_us_fol =df_us_follower.iloc[:25,:]\n",
    "df_count_us_fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_us_follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis.groupby('category')['person_followers_count'].mean()  \n",
    "# mean number of followers by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fol_dep=df_us_follower.loc[df_us_follower['category']=='depressed'] \n",
    "#Number of highest followers in depressed category\n",
    "df_fol_dep.sort_values(by=['person_followers_count'])\n",
    "df_fol_dep['person_followers_count'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fol_happy=df_us_follower.loc[df_us_follower['category']=='non_depressed'] \n",
    "#Number of highest followers in non depressed category\n",
    "df_fol_happy.sort_values(by=['person_followers_count'])\n",
    "df_fol_happy['person_followers_count'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_plot(x_=df_count_us_fol.person_followers_count,y_=df_count_us_fol.username,z_=df_count_us_fol.category,xlabel_='count',title_='25 users with maximum following count',fig_size_=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creation of Age Variable</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['account_year']=pd.DatetimeIndex(df_vis['account_creation_date']).year  \n",
    "#creation of age variable\n",
    "df_vis['age']= 2020-df_vis[\"account_year\"]+16 #default age =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Depressed users based on age </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df_vis['age'],columns = df_vis['category']).plot(kind='bar',color=['red','blue'],figsize=(8, 6),alpha=0.6,rot=0,title=\"Depressed and Non Depressed Users based on age\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Twitter Posts Based on Age </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_age=pd.crosstab(df_vis.age,df_vis.category)\n",
    "posts_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_vc = df_vis.groupby(['hour']).age.value_counts() \n",
    "hours_vc.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Graph Showing Correlation between age and day-hour of twitter post</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_graph = hours_vc.unstack().plot(kind='line',marker='o',linestyle='dashed',markersize=7,color=['b','#95a5a6','r','c','y','g','m','#E67E22','#e74c3c','#34495e','#839192', '#1a000d','#77ff33','#b300b3','#264d00'],figsize=(15, 10),rot=0,title=\"Number of Users by following age groups posted in different times\")\n",
    "hour_graph.set_xlabel(\"Time\")\n",
    "hour_graph.set_ylabel(\"Number of users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_dep = df_vis[df_vis['category']=='depressed']\n",
    "df_negative_dep = df_vis[df_vis['category']=='non-depressed']\n",
    "tweet_dep = \" \".join(review for review in df_positive_dep['tweet'])\n",
    "tweet_neg = \" \".join(review for review in df_negative_dep['tweet'])\n",
    "\n",
    "\n",
    "imgg_all, axx_all = plt.subplots(2, 1, figsize  = (12,12)) #initializing image plot for word cloud\n",
    "\n",
    "positive_text_dep = WordCloud(max_font_size=40, max_words=100, background_color=\"white\",colormap=matplotlib.cm.magma).generate(tweet_dep)\n",
    "negative_text_dep = WordCloud(max_font_size=40, max_words=100, background_color=\"white\", colormap=matplotlib.cm.magma).generate(tweet_neg)\n",
    "\n",
    "axx_all[0].imshow(positive_text_dep, interpolation='bilinear')\n",
    "axx_all[0].set_title('Tweets under Depression Class',fontsize=20)\n",
    "axx_all[0].axis('off')\n",
    "axx_all[1].imshow(negative_text_dep, interpolation='bilinear')\n",
    "axx_all[1].set_title('Tweets under Non Depression Class',fontsize=20)\n",
    "axx_all[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preparing for Data Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.dropna(subset=['tweet'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Cleaning and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tolist=all_tweets['tweet'].tolist()  #storing tweets column from dataframe into list\n",
    "tweets_tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_tolist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(document):\n",
    "    punctuation_free_text=\"\".join([word for word in document if word not in string.punctuation])\n",
    "    return punctuation_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(document):\n",
    "    initialize_soup=BeautifulSoup(document,'lxml')\n",
    "    tags_free_context=initialize_soup.get_text()\n",
    "    return tags_free_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_of_stopwords(document):\n",
    "    context=[word for word in document if word not in stop_words]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_of_links(document):\n",
    "    text = re.sub(r\"http\\S+\", \"\", document, flags=re.MULTILINE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(document):\n",
    "    return document.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras_list=['i','like','they','whom','what','where','my','ive','have','im','maybe','would','whether',1,2,3,4,5,2019,'when','They','I','took',\n",
    "            'It','and','And','mom','dad','okay','Okay','say','Still','mother','father','Someone','someone','The','the','youre','just','Just',\n",
    "             'dont','whats','even','yours','Ive','gets','This','this','bc','dm','he','He','she','She','5th','pic','twitter','Twitter',\n",
    "             'eVgHWmsmvr','etc','90s','w','since','v','tH7zzf4Gqj','a2vse3xkjh','hi','name','Hi','com' ,'dagmyoikkx','alhamdulilah','rn','to',\n",
    "             'To','gon','par','so','10','20',10,20,'ok','Ok','OK','emdr','u','we'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_words(document):\n",
    "    context=[]\n",
    "    #print(\"extras_list \\n\",extras_list)\n",
    "    for word in document:\n",
    "        #print(word)\n",
    "        if word not in extras_list:\n",
    "            context.append(word)\n",
    "            #print(\"word \",word)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0     #performing all steps of data cleaning\n",
    "for tweet in tweets_tolist:\n",
    "    print(c)\n",
    "    c=c+1\n",
    "    cleaned_sentence=removal_of_links(tweet) #calling functions required for data cleaning\n",
    "    cleaned_sentence=remove_emojis(cleaned_sentence)\n",
    "    cleaned_sentence=remove_html_tags(cleaned_sentence)\n",
    "    cleaned_sentence=remove_punctuation_marks(cleaned_sentence)\n",
    "    tokens = nltk.word_tokenize(cleaned_sentence)\n",
    "    cleaned_context_tt=removal_of_stopwords(tokens)\n",
    "    cleaned_context=remove_extra_words(cleaned_context_tt)\n",
    "    filtered_sentence = []\n",
    "    for w in cleaned_context:\n",
    "        filtered_sentence.append(w.lower())\n",
    "    cleaned_context = ' '.join([item for item in filtered_sentence])   \n",
    "    cleaned_tweets.append(cleaned_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tweets Before and After Cleaning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tolist[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Topic Modeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Topic Modeling on Depressed Tweets </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed.shape\n",
    "df_depressed.dropna(subset=['tweet'],inplace=True) #checking for null tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressedtweets_tolist=df_depressed['tweet'].tolist() #converting tweets into list\n",
    "depressedtweets_tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_cleaned_tweets=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0    #cleaning depressed tweets separately\n",
    "for tweet in depressedtweets_tolist:\n",
    "    cleaned_sentence=removal_of_links(tweet)\n",
    "    cleaned_sentence=remove_emojis(cleaned_sentence)\n",
    "    cleaned_sentence=remove_html_tags(cleaned_sentence)\n",
    "    cleaned_sentence=remove_punctuation_marks(cleaned_sentence)\n",
    "    tokens = nltk.word_tokenize(cleaned_sentence)\n",
    "    cleaned_context=removal_of_stopwords(tokens)\n",
    "    filtered_sentence = []\n",
    "    for w in cleaned_context:\n",
    "        filtered_sentence.append(w.lower())\n",
    "    cleaned_context = ' '.join([item for item in filtered_sentence])     \n",
    "    depressed_cleaned_tweets.append(cleaned_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(depressed_cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_cleaned_tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(depressed_cleaned_tweets[5601])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_words(tweets):  #generating tokens from text\n",
    "    for tweet in tweets:\n",
    "        yield(gensim.utils.simple_preprocess(str(tweet)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_tweet_words = list(extracting_words(depressed_cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_id_to_word = corpora.Dictionary(dep_tweet_words) #creating dictionary for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_text = dep_tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_corpus = [dep_id_to_word.doc2bow(text) for text in dep_text] #creating corpus for depression related keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_lda_model = gensim.models.ldamodel.LdaModel(corpus=dep_corpus,  #initializing lda model\n",
    "                                           id2word=dep_id_to_word,chunksize=100,\n",
    "                                           num_topics=12,\n",
    "                                                passes=10, random_state=100,\n",
    "                                                update_every=1,\n",
    "                                           per_word_topics=True,alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook() #showing lda topics in notebook\n",
    "dep_vis = pyLDAvis.gensim.prepare(dep_lda_model, dep_corpus, dep_id_to_word) #visualizing lda model\n",
    "dep_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Happy Tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_happy.dropna(subset=['tweet'],inplace=True) #checking null values for happy tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "happytweets_tolist=df_tweets_happy['tweet'].tolist() #storing tweets in list\n",
    "happytweets_tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_cleaned_tweets=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in happytweets_tolist:                #Performing data cleaning on happy tweets\n",
    "    cleaned_sentence=removal_of_links(tweet)\n",
    "    cleaned_sentence=remove_emojis(cleaned_sentence)\n",
    "    cleaned_sentence=remove_html_tags(cleaned_sentence)\n",
    "    cleaned_sentence=remove_punctuation_marks(cleaned_sentence)\n",
    "    tokens = nltk.word_tokenize(cleaned_sentence)\n",
    "    cleaned_context=removal_of_stopwords(tokens)\n",
    "    filtered_sentence = []\n",
    "    for w in cleaned_context:\n",
    "        filtered_sentence.append(w.lower())\n",
    "    cleaned_context = ' '.join([item for item in filtered_sentence])     \n",
    "    happy_cleaned_tweets.append(cleaned_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(happy_cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_cleaned_tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_tweet_words = list(extracting_words(happy_cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_id_to_word = corpora.Dictionary(happy_tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_text = happy_tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_corpus = [happy_id_to_word.doc2bow(text) for text in happy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_lda_model = gensim.models.ldamodel.LdaModel(corpus=happy_corpus,\n",
    "                                           id2word=happy_id_to_word,chunksize=100,\n",
    "                                           num_topics=20,passes=10, \n",
    "                                            random_state=100,update_every=1,\n",
    "                                           per_word_topics=True,\n",
    "                                            alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_topics = happy_lda_model.show_topics(formatted=False)\n",
    "happy_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook() #visualizing topics in happy tweets\n",
    "happy_vis = pyLDAvis.gensim.prepare(happy_lda_model, happy_corpus, happy_id_to_word)\n",
    "happy_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE TWEET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tolist[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Conversion of Words into Vectors</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TFIDF </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words=[]\n",
    "for i in range(0,len(cleaned_tweets)):\n",
    "    tokens = nltk.word_tokenize(cleaned_tweets[i])\n",
    "    for w in tokens:\n",
    "        total_words.append(w)\n",
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=Counter(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={j:i for i,j in enumerate(counts.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = TfidfVectorizer( vocabulary=vocab)\n",
    "tweets_matrix = vectorize.fit_transform(cleaned_tweets)\n",
    "tweets_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = csr_matrix(tweets_matrix[4])\n",
    "print(\"TF-IDF Sparse matrix: \\n\",S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_matrix[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets_matrix[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = csr_matrix(tweets_matrix[1])\n",
    "#B = S.todense()\n",
    "#print(\"Dense matrix: \\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Loading Glove Embeddings File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "words_l=[]\n",
    "i=0\n",
    "file_link = \"glove.6B/glove.6B.50d.txt\"\n",
    "def load_glove_Model_processing(file_link):\n",
    "    with io.open(file_link, encoding=\"utf8\" ) as ff:\n",
    "        content_l = ff.readlines()\n",
    "    model_dict = {}\n",
    "    for l in content_l:\n",
    "        split_Line = l.split()\n",
    "        word_l = split_Line[0]\n",
    "        embedding_l = np.array([float(val) for val in split_Line[1:]])\n",
    "        model_dict[word_l] = embedding_l\n",
    "        words_l.append(word_l)\n",
    "    print (len(model_dict),\" words loaded!\")\n",
    "    return model_dict\n",
    "\n",
    "model= load_glove_Model_processing(file_link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores={}\n",
    "for i in range(0,len(cleaned_tweets)):\n",
    "    tokens= nltk.word_tokenize(cleaned_tweets[i])\n",
    "    for w in tokens:\n",
    "        try :\n",
    "            scores[w]=np.array(model[w])\n",
    "        except : \n",
    "            scores[w]=np.zeros((1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_Matrix=[np.reshape(scores[i],(1,50)) for i in scores.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix_embeddings=np.reshape(Word_Matrix,(len(Word_Matrix),50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_matrix_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_matrix_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = csr_matrix(word_matrix_embeddings[4])\n",
    "B = S.todense()\n",
    "print(\"glove embeddings: \\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tweets_tfidf_matrix = sparse.csr_matrix(tweets_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tweets_tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_word_matrix = sparse.csr_matrix(word_matrix_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = csr_matrix(sparse_word_matrix[3])\n",
    "#B = S.todense()\n",
    "#print(\"Sparse word matrix: \\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Glove Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweets_matrix=np.dot(sparse_tweets_tfidf_matrix,sparse_word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweets_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweets_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = csr_matrix(final_tweets_matrix[4])\n",
    "B = S.todense()\n",
    "print(\"Glove + TF-IDF: \\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = csr_matrix(final_tweets_matrix[1])\n",
    "#print(\"Sparse matrix: \\n\",S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training Word2Vec Embeddings from Gensim </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in range(len(cleaned_tweets)) :\n",
    "    sentences.append(cleaned_tweets[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec(sentences,size=50,window=10,min_count=1,workers=10,iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_word2vec = list(word_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.save('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model_trained = Word2Vec.load('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model_trained['depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = csr_matrix(word_model_trained[1])\n",
    "#print(\"Sparse matrix: \\n\",S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Combining Word2vec with Tfidf </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_scores={}\n",
    "for i in range(0,len(cleaned_tweets)):\n",
    "    tokens= nltk.word_tokenize(cleaned_tweets[i])\n",
    "    for w in tokens:\n",
    "        try :\n",
    "            word_vec_scores[w]=np.array(word_model[w])\n",
    "        except : \n",
    "            word_vec_scores[w]=np.zeros((1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_vector_Matrix=[np.reshape(word_vec_scores[i],(1,50)) for i in word_vec_scores.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_embeddings=np.reshape(Word_vector_Matrix,(len(Word_vector_Matrix),50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = csr_matrix(word_vector_embeddings[4])\n",
    "B = S.todense()\n",
    "print(\"word2vec embeddings: \\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_word_vectors = sparse.csr_matrix(word_vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_tweets_matrix=np.dot(sparse_tweets_tfidf_matrix,sparse_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_tweets_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_tweets_matrix[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = csr_matrix(wordvec_tweets_matrix[4])\n",
    "B = S.todense()\n",
    "print(\"Word2Vec + TF-IDF: \\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Application of Models using 10 fold cross validation with Glove and Tfidf</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(final_tweets_matrix, all_tweets['target'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Random Forest using Glove Embeddings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_f=RandomForestClassifier()\n",
    "start_time=time.time()\n",
    "r_f.fit(X_train,Y_train)\n",
    "value_pred_r_f = r_f.predict(X_test)\n",
    "scores_r_f=cross_val_score(r_f, X_test,Y_test, cv=10)\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for random forest is \\n\",confusion_matrix(Y_test,value_pred_r_f))\n",
    "print(\"Accuracy after applying random forest\",(accuracy_score(Y_test,value_pred_r_f)))\n",
    "print(\"Scores after applying 10 CV on random forest\",np.average(scores_r_f))\n",
    "print(\"Precision for random forest \",precision_score(Y_test,value_pred_r_f))\n",
    "print(\"Recall for random forest \",recall_score(Y_test,value_pred_r_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Random Forest without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Random Forest\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(r_f, X_test, Y_test, display_labels=class_labels,cmap=plt.cm.Blues,normalize=nz)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf, tpr_rf, threshold_rf = metrics.roc_curve(Y_test, value_pred_r_f)\n",
    "roc_auc_rf = metrics.auc(fpr_rf, tpr_rf)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Random Forest Using Glove Embeddings')\n",
    "plt.plot(fpr_rf, tpr_rf, '', label = 'AUC = %0.2f' % roc_auc_rf)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='blue')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Support Vector Machine Classifier using Glove Embeddings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_svm=svm.SVC(C=1.0, kernel='rbf', degree=3)\n",
    "start_time=time.time()\n",
    "load_svm.fit(X_train,Y_train)\n",
    "value_pred_svm = load_svm.predict(X_test)\n",
    "scores_svm=cross_val_score(load_svm, X_test,Y_test, cv=10)\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Support Vector Machine Classifier is \\n\",confusion_matrix(Y_test,value_pred_svm))\n",
    "print(\"Accuracy after applying Support Vector Machine is\",(accuracy_score(Y_test,value_pred_svm)))\n",
    "print(\"Scores after applying 10 CV on Support Vector Machine\",np.average(scores_svm))\n",
    "print(\"Precision for Support Vector Machine \",precision_score(Y_test,value_pred_svm))\n",
    "print(\"Recall for Support Vector Machine \",recall_score(Y_test,value_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Support Vector Machine without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Support Vector Machine\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(load_svm, X_test, Y_test,display_labels=class_labels,cmap=plt.cm.Reds,normalize=nz)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_svm, tpr_svm, threshold_svm = metrics.roc_curve(Y_test, value_pred_svm)\n",
    "roc_auc_svm = metrics.auc(fpr_svm, tpr_svm)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Support Vector Machine Using Glove Embeddings')\n",
    "plt.plot(fpr_svm, tpr_svm, '', label = 'AUC = %0.2f' % roc_auc_svm)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='red')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression using Glove Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_reg=LogisticRegression(penalty='l2',random_state=None)\n",
    "start_time=time.time()\n",
    "load_log_reg.fit(X_train,Y_train)\n",
    "value_pred_log = load_log_reg.predict(X_test)\n",
    "scores_log_reg=cross_val_score(load_log_reg, X_test,Y_test, cv=10) \n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Logistic Regression is \\n\",confusion_matrix(Y_test,value_pred_log))\n",
    "print(\"Accuracy after applying Logistic Regression is\",(accuracy_score(Y_test,value_pred_log)))\n",
    "print(\"Scores after applying 10 CV on Logistic Regression\",np.average(scores_log_reg))\n",
    "print(\"Precision for Logistic Regression \",precision_score(Y_test,value_pred_log))\n",
    "print(\"Recall for Logistic Regression \",recall_score(Y_test,value_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Logistic Regression without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Logistic Regression\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(load_log_reg, X_test, Y_test,display_labels=class_labels,normalize=nz,cmap=plt.cm.Oranges)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_log, tpr_log, threshold_log = metrics.roc_curve(Y_test, value_pred_log)\n",
    "roc_auc_log_reg = metrics.auc(fpr_log, tpr_log)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Logistic Rgression Using Glove Embeddings')\n",
    "plt.plot(fpr_log, tpr_log, '', label = 'AUC = %0.2f' % roc_auc_log_reg)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='orange')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> XGBoost using Glove Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xg_boost = xgb.XGBClassifier(gamma=0.1,base_estimator= r_f, objective='binary:logistic', learning_rate=0.01, max_depth=10, n_estimators=200, random_state=1)\n",
    "load_xg_boost.fit(X_train, Y_train)\n",
    "val_pred_xg_boost = load_xg_boost.predict(X_test)\n",
    "scores_xg_boost=cross_val_score(load_xg_boost, X_test,Y_test, cv=10)\n",
    "xg_boost_predictions = [round(value) for value in val_pred_xg_boost]\n",
    "print(\"Confusion Matrix obtained for XG Boost is \\n\",confusion_matrix(Y_test,xg_boost_predictions))\n",
    "print(\"Accuracy after applying XG Boost\",(accuracy_score(Y_test,xg_boost_predictions)))\n",
    "print(\"Scores after applying 10 CV on XG Boost\",np.average(scores_xg_boost))\n",
    "print(\"Precision for XG Boost \",precision_score(Y_test,xg_boost_predictions))\n",
    "print(\"Recall for XG Boost \",recall_score(Y_test,xg_boost_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_xg_boost, tpr_xg_boost, threshold_xg_boost = metrics.roc_curve(Y_test, xg_boost_predictions)\n",
    "roc_auc_xg_boost = metrics.auc(fpr_xg_boost, tpr_xg_boost)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For XG Boost Using Glove Embeddings')\n",
    "plt.plot(fpr_xg_boost, tpr_xg_boost, '', label = 'AUC = %0.2f' % roc_auc_xg_boost)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='brown')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Adaboost using Glove Embeddings  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_adb = AdaBoostClassifier() #default learner is decision trees\n",
    "start_time=time.time()\n",
    "load_adb.fit(X_train,Y_train)\n",
    "val_pred_adb = load_adb.predict(X_test)\n",
    "scores_adb_=cross_val_score(load_adb, X_test,Y_test, cv=10)\n",
    "adb_predictions = [round(value) for value in val_pred_adb]\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Adaboost is \\n\",confusion_matrix(Y_test,adb_predictions))\n",
    "print(\"Accuracy after applying Adaboost\",(accuracy_score(Y_test,adb_predictions)))\n",
    "print(\"Scores after applying 10 CV on Adaboost\",np.average(scores_adb_))\n",
    "print(\"Precision for AdaBoost \",precision_score(Y_test,adb_predictions))\n",
    "print(\"Recall for AdaBoost \",recall_score(Y_test,adb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_adb, tpr_adb, thresholdadb = metrics.roc_curve(Y_test, adb_predictions)\n",
    "roc_auc_adb = metrics.auc(fpr_adb, tpr_adb)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For AdaBoost Using Glove Embeddings')\n",
    "plt.plot(fpr_adb, tpr_adb, '', label = 'AUC = %0.2f' % roc_auc_adb)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='purple')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Gradient Boosting using GLOVE Embeddings  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gdb = GradientBoostingClassifier(n_estimators=100)\n",
    "start_time=time.time()\n",
    "load_gdb.fit(X_train,Y_train)\n",
    "value_pred_gdb = load_gdb.predict(X_test)\n",
    "scores_gdb_=cross_val_score(load_gdb, X_test,Y_test, cv=10)\n",
    "gdb_predictions = [round(value) for value in value_pred_gdb]\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Gradient Boosting is \\n\",confusion_matrix(Y_test,gdb_predictions))\n",
    "print(\"Accuracy after applying Gradient Boosting\",(accuracy_score(Y_test,gdb_predictions)))\n",
    "print(\"Scores after applying 10 CV on Gradient Boosting\",np.average(scores_gdb_))\n",
    "print(\"Precision for Gradient Boosting \",precision_score(Y_test,adb_predictions))\n",
    "print(\"Recall for Gradient Boosting \",recall_score(Y_test,adb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_gdb, tpr_gdb, thresholdgdb = metrics.roc_curve(Y_test, gdb_predictions)\n",
    "roc_auc_gdb = metrics.auc(fpr_gdb, tpr_gdb)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Gradient Boosting Using Glove Embeddings')\n",
    "plt.plot(fpr_gdb, tpr_gdb, '', label = 'AUC = %0.2f' % roc_auc_gdb)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='black')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ROC Curve for all algorithms </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0).clf() #creating figure\n",
    "plt.plot(fpr_rf, tpr_rf, '', label = 'AUC = %0.2f Random Forest' % roc_auc_rf)\n",
    "plt.plot(fpr_svm, tpr_svm, '', label = 'AUC = %0.2f SVM' % roc_auc_svm)\n",
    "plt.plot(fpr_log, tpr_log, '', label = 'AUC = %0.2f Logistic Regression' % roc_auc_log_reg)\n",
    "plt.plot(fpr_xg_boost, tpr_xg_boost, '', label = 'AUC = %0.2f XG Boost' % roc_auc_xg_boost)\n",
    "plt.plot(fpr_adb, tpr_adb, '', label = 'AUC = %0.2f AdaBoost' % roc_auc_adb)\n",
    "plt.plot(fpr_gdb, tpr_gdb, '', label = 'AUC = %0.2f Gradient Boost' % roc_auc_gdb)\n",
    "plt.plot()\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> BiLSTM Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS_check = len(counts) #creating check of maximum words\n",
    "tokenizer_check= Tokenizer(num_words=MAX_NUM_WORDS_check) #tokenizing context\n",
    "tokenizer_check.fit_on_texts(cleaned_tweets)\n",
    "word_vector_obtained = tokenizer_check.texts_to_sequences(cleaned_tweets)\n",
    "word_index_obtained = tokenizer_check.word_index\n",
    "vocab_size_check = len(word_index_obtained) #checking size of vocab\n",
    "vocab_size_check #printing vocab for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH_CHECK = 10  #defining sequence length\n",
    "input_tensor_obtained = pad_sequences(word_vector_obtained, maxlen=MAX_SEQ_LENGTH_CHECK) #creating input tensor\n",
    "print(input_tensor_obtained.shape)\n",
    "EMBEDDING_DIMENSIONS = 50 #defining shape of embeddings\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS_check, EMBEDDING_DIMENSIONS)) #creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_obtained = Input(shape=(MAX_SEQ_LENGTH_CHECK,)) #initializing neural nets\n",
    "x_obtained = Embedding(MAX_NUM_WORDS_check, EMBEDDING_DIMENSIONS, weights=[word_matrix_embeddings])(input_obtained)\n",
    "x_obtained = Bidirectional(LSTM(10 , recurrent_dropout=0.01,return_sequences=True,dropout=0.20))(x_obtained)\n",
    "x_obtained = GlobalMaxPool1D()(x_obtained)#[preparing inputs]\n",
    "x_obtained = Dense(10, activation=\"relu\")(x_obtained)#applying relu effect\n",
    "x_obtained = Dropout(0.20)(x_obtained)\n",
    "x_obtained = Dense(1, activation=\"sigmoid\")(x_obtained)#applying sigmoid reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Model(inputs=input_obtained, outputs=x_obtained)\n",
    "model_lstm.compile(optimizer='adam', metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input_tensor_obtained, all_tweets['target'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(x_train, y_train, batch_size=8, epochs=5) #fitting neural nets on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_lstm = model_lstm.predict(x_test)\n",
    "preds_lstm  = np.round(preds_lstm.flatten())\n",
    "print(classification_report(y_test, preds_lstm, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Long Short Term Memory</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_load = Sequential() #loading lstm for sequential classification \n",
    "model_lstm_load.add(Embedding(len(word_matrix_embeddings), \n",
    "                               EMBEDDING_DIMENSIONS, weights=[word_matrix_embeddings], \n",
    "                            input_length=10, trainable=False))\n",
    "model_lstm_load.add(Conv1D(kernel_size=3, filters=32, activation='relu',padding='same'))\n",
    "model_lstm_load.add(MaxPooling1D(pool_size=2)) #adding pool size\n",
    "model_lstm_load.add(Dropout(0.2)) #selecting parameter for dropout\n",
    "model_lstm_load.add(LSTM(300)) #add layers\n",
    "model_lstm_load.add(Dropout(0.2)) #again defining dropout\n",
    "model_lstm_load.add(Dense(1, activation='sigmoid')) #adding sigmoid parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm_load.compile(loss='binary_crossentropy',  metrics=['acc'],optimizer='nadam')\n",
    "print(model_lstm_load.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=5 #defining number of epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3) #defining early stop\n",
    "model_seq_hist = model_lstm_load.fit(x_train, y_train,epochs=EPOCHS, batch_size=12,shuffle=True,callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_seq = model_lstm_load.predict(x_test)\n",
    "labels_pred_seq = np.round(labels_pred_seq.flatten())\n",
    "accuracy_seq = accuracy_score(y_test, labels_pred_seq)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_seq*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, labels_pred_seq, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Trained Word2Vec Vectors </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wordvec_tweets_matrix, all_tweets['target'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Random Forest using Trained Word2Vec </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_f=RandomForestClassifier()\n",
    "start_time=time.time()\n",
    "r_f.fit(X_train,Y_train)\n",
    "value_pred_r_f = r_f.predict(X_test)\n",
    "scores_r_f=cross_val_score(r_f, X_test,Y_test, cv=10) \n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for random forest is \\n\",confusion_matrix(Y_test,value_pred_r_f))\n",
    "print(\"Accuracy after applying random forest\",(accuracy_score(Y_test,value_pred_r_f)))\n",
    "print(\"Scores after applying 10 CV on random forest\",np.average(scores_r_f))\n",
    "print(\"Precision for random forest \",precision_score(Y_test,value_pred_r_f))\n",
    "print(\"Recall for random forest \",recall_score(Y_test,value_pred_r_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Random Forest without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Random Forest\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(r_f, X_test, Y_test, display_labels=class_labels,cmap=plt.cm.Blues,normalize=nz)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf, tpr_rf, threshold_rf = metrics.roc_curve(Y_test, value_pred_r_f)\n",
    "roc_auc_rf = metrics.auc(fpr_rf, tpr_rf)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Random Forest Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_rf, tpr_rf, '', label = 'AUC = %0.2f' % roc_auc_rf)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='blue')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SVM using Trained Word2Vec </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_svm=svm.SVC(C=1.0, kernel='rbf', degree=3)\n",
    "start_time=time.time()\n",
    "load_svm.fit(X_train,Y_train)\n",
    "value_pred_svm = load_svm.predict(X_test)\n",
    "scores_svm=cross_val_score(load_svm, X_test,Y_test, cv=10) \n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Support Vector Machine Classifier is \\n\",confusion_matrix(Y_test,value_pred_svm))\n",
    "print(\"Accuracy after applying Support Vector Machine is\",(accuracy_score(Y_test,value_pred_svm)))\n",
    "print(\"Scores after applying 10 CV on Support Vector Machine\",np.average(scores_svm))\n",
    "print(\"Precision for Support Vector Machine \",precision_score(Y_test,value_pred_svm))\n",
    "print(\"Recall for Support Vector Machine \",recall_score(Y_test,value_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Support Vector Machine without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Support Vector Machine\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(load_svm, X_test, Y_test,display_labels=class_labels,cmap=plt.cm.Reds,normalize=nz)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_svm, tpr_svm, threshold_svm = metrics.roc_curve(Y_test, value_pred_svm)\n",
    "roc_auc_svm = metrics.auc(fpr_svm, tpr_svm)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Support Vector Machine Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_svm, tpr_svm, '', label = 'AUC = %0.2f' % roc_auc_svm)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='red')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression using Trained Word2Vec  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_reg=LogisticRegression(penalty='l2',random_state=None)\n",
    "start_time=time.time()\n",
    "load_log_reg.fit(X_train,Y_train)\n",
    "value_pred_log = load_log_reg.predict(X_test)\n",
    "scores_log_reg=cross_val_score(load_log_reg, X_test,Y_test, cv=10) \n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Logistic Regression is \\n\",confusion_matrix(Y_test,value_pred_log))\n",
    "print(\"Accuracy after applying Logistic Regression is\",(accuracy_score(Y_test,value_pred_log)))\n",
    "print(\"Scores after applying 10 CV on Logistic Regression\",np.average(scores_log_reg))\n",
    "print(\"Precision for Logistic Regression \",precision_score(Y_test,value_pred_log))\n",
    "print(\"Recall for Logistic Regression \",recall_score(Y_test,value_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels=[0,1]\n",
    "titles = [(\"Printing Confusion matrix For Logistic Regression without normalization\", None),\n",
    "          (\"Printing Normalized confusion matrix for Logistic Regression\", 'true')]\n",
    "for t, nz in titles:\n",
    "    display = plot_confusion_matrix(load_log_reg, X_test, Y_test,display_labels=class_labels,\n",
    "                                 normalize=nz,cmap=plt.cm.Oranges)\n",
    "    display.ax_.set_title(t)\n",
    "    print(t)\n",
    "    print(display.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_log, tpr_log, threshold_log = metrics.roc_curve(Y_test, value_pred_log)\n",
    "roc_auc_log_reg = metrics.auc(fpr_log, tpr_log)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Logistic Rgression Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_log, tpr_log, '', label = 'AUC = %0.2f' % roc_auc_log_reg)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='orange')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])#values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> AdaBoost Classifier using Trained Word2Vec  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_adb = AdaBoostClassifier() # default weak learner is decision trees\n",
    "start_time=time.time()\n",
    "load_adb.fit(X_train,Y_train)\n",
    "val_pred_adb = load_adb.predict(X_test)\n",
    "scores_adb_=cross_val_score(load_adb, X_test,Y_test, cv=10)\n",
    "adb_predictions = [round(value) for value in val_pred_adb]\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Adaboost is \\n\",confusion_matrix(Y_test,adb_predictions))\n",
    "print(\"Accuracy after applying Adaboost\",(accuracy_score(Y_test,adb_predictions)))\n",
    "print(\"Scores after applying 10 CV on Adaboost\",np.average(scores_adb_))\n",
    "print(\"Precision for AdaBoost \",precision_score(Y_test,adb_predictions))\n",
    "print(\"Recall for AdaBoost \",recall_score(Y_test,adb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_adb, tpr_adb, thresholdadb = metrics.roc_curve(Y_test, adb_predictions)\n",
    "roc_auc_adb = metrics.auc(fpr_adb, tpr_adb)\n",
    "plt.title('Receiver Operating Characteristic For AdaBoost Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_adb, tpr_adb, '', label = 'AUC = %0.2f' % roc_auc_adb)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='purple')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Gradient Boosting Classifier using Trained Word2Vec  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gdb = GradientBoostingClassifier(n_estimators=100)\n",
    "start_time=time.time()\n",
    "load_gdb.fit(X_train,Y_train)\n",
    "value_pred_gdb = load_gdb.predict(X_test)\n",
    "scores_gdb_=cross_val_score(load_gdb, X_test,Y_test, cv=10)\n",
    "gdb_predictions = [round(value) for value in value_pred_gdb]\n",
    "print(\"Finished in ... \",time.time()-start_time)\n",
    "print(\"Confusion Matrix obtained for Gradient Boosting is \\n\",confusion_matrix(Y_test,gdb_predictions))\n",
    "print(\"Accuracy after applying Gradient Boosting\",(accuracy_score(Y_test,gdb_predictions)))\n",
    "print(\"Scores after applying 10 CV on Gradient Boosting\",np.average(scores_gdb_))\n",
    "print(\"Precision for Gradient Boosting \",precision_score(Y_test,adb_predictions))\n",
    "print(\"Recall for Gradient Boosting \",recall_score(Y_test,adb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_gdb, tpr_gdb, thresholdgdb = metrics.roc_curve(Y_test, gdb_predictions)\n",
    "roc_auc_gdb = metrics.auc(fpr_gdb, tpr_gdb)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For Gradient Boosting Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_gdb, tpr_gdb, '', label = 'AUC = %0.2f' % roc_auc_gdb)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='black')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> XGBoost Classifier using Trained Word2Vec  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xg_boost = xgb.XGBRegressor(gamma=0.1,base_estimator= r_f, objective='binary:logistic', learning_rate=0.01, max_depth=10, n_estimators=200, random_state=1)\n",
    "load_xg_boost.fit(X_train, Y_train)\n",
    "val_pred_xg_boost = load_xg_boost.predict(X_test)\n",
    "scores_xg_boost=cross_val_score(load_xg_boost, X_test,Y_test, cv=10)\n",
    "xg_boost_predictions = [round(value) for value in val_pred_xg_boost]\n",
    "print(\"Confusion Matrix obtained for XG Boost is \\n\",confusion_matrix(Y_test,xg_boost_predictions))\n",
    "print(\"Accuracy after applying XG Boost\",(accuracy_score(Y_test,xg_boost_predictions)))\n",
    "print(\"Scores after applying 10 CV on XG Boost\",np.average(scores_xg_boost))\n",
    "print(\"Precision for XG Boost \",precision_score(Y_test,xg_boost_predictions))\n",
    "print(\"Recall for XG Boost \",recall_score(Y_test,xg_boost_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_xg_boost, tpr_xg_boost, threshold_xg_boost = metrics.roc_curve(Y_test, xg_boost_predictions)\n",
    "roc_auc_xg_boost = metrics.auc(fpr_xg_boost, tpr_xg_boost)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic For XG Boost Using Word2Vec Embeddings')\n",
    "plt.plot(fpr_xg_boost, tpr_xg_boost, '', label = 'AUC = %0.2f' % roc_auc_xg_boost)\n",
    "plt.legend(loc = 'center right')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--',color='brown')\n",
    "plt.xlim([0, 1])   #values for x limit\n",
    "plt.ylim([0, 1])   #values for y limit\n",
    "plt.ylabel('Values for True Positive Rate')\n",
    "plt.xlabel('Values for False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Combined ROC Curve for Word2Vec Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0).clf() #creating figure\n",
    "plt.plot(fpr_rf, tpr_rf, '', label = 'AUC = %0.2f Random Forest' % roc_auc_rf)\n",
    "plt.plot(fpr_svm, tpr_svm, '', label = 'AUC = %0.2f SVM' % roc_auc_svm)\n",
    "plt.plot(fpr_log, tpr_log, '', label = 'AUC = %0.2f Logistic Regression' % roc_auc_log_reg)\n",
    "plt.plot(fpr_xg_boost, tpr_xg_boost, '', label = 'AUC = %0.2f XG Boost' % roc_auc_xg_boost)\n",
    "plt.plot(fpr_adb, tpr_adb, '', label = 'AUC = %0.2f AdaBoost' % roc_auc_adb)\n",
    "plt.plot(fpr_gdb, tpr_gdb, '', label = 'AUC = %0.2f Gradient Boost' % roc_auc_gdb)\n",
    "plt.plot()\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
